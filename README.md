# Web Scraping Tutor - Jira Scraper & LLM Dataset Generator

A fault-tolerant data pipeline that scrapes issue data from Apache Jira and transforms it into JSONL format for LLM training.

## Overview

This project:
1. Scrapes issues from 3 Apache Jira projects (KAFKA, SPARK, HADOOP)
2. Handles network failures, rate limits, and data inconsistencies
3. Transforms raw data into structured JSONL format with 4 task types per issue
4. Resumes from checkpoint if interrupted (zero data loss)

---

## Architecture
<pre>
ğŸ“¦ Scraper (scraper.py)
   â†“ (Raw JSON, 100 issues/batch)
ğŸ’¾ Checkpoint Manager (checkpoint_manager.py) â† saves progress
   â†“
ğŸ“ data/raw/ (batch files)
   â†“
âš™ï¸ Transformer (transformer.py) â† 4 tasks per issue
   â†“
ğŸ—‚ï¸ data/processed/ (JSONL files)
</pre>




### Components

**config.py** - Centralized settings (projects, batch size, retry limits)

**checkpoint_manager.py** - Saves/loads progress to JSON
- Method: `save_checkpoint()` - saves position after each batch
- Method: `load_checkpoint()` - resumes from last position

**scraper.py** - Jira REST API integration
- Uses `requests.Session()` for connection reuse
- `@retry` decorator with exponential backoff (2s â†’ 60s)
- Special handling for HTTP 429 (rate limit): wait 60s, then retry
- Fetches 100 issues per request (batch pagination)

**transformer.py** - Converts raw data to JSONL
- Creates 4 training examples per issue:
  1. Summarization (description â†’ title)
  2. Status classification (title+desc â†’ status)
  3. Priority classification (title+desc â†’ priority)
  4. Question-answering (context+question â†’ answer)
- Includes complete metadata: issue_key, project, summary, status, priority, assignee, reporter, created, updated, resolutiondate, labels

**main.py** - Orchestrates scraper â†’ transformer pipeline
- CLI args: `--scrape-only`, `--transform-only`
- Error handling and progress reporting

---

## Setup

### Prerequisites
- Python 3.8+
- pip
- Internet connection
- ~1GB disk space

### Installation

git clone https://github.com/nimishgiras123/webScrapingTutor.git
cd webScrapingTutor

python -m venv venv
source venv/bin/activate # On Windows: venv\Scripts\activate

pip install -r requirements.txt


### No configuration needed
Apache Jira is publicly accessible at `https://issues.apache.org/jira/`

Optional: Edit `config.py` to change projects, batch size, or retry limits.

---

## Usage

Full pipeline (scrape + transform)
python main.py

Scrape only
python main.py --scrape-only

Transform only
python main.py --transform-only

Resume automatically - if interrupted, just run again
python main.py


### Output
- `data/raw/` - Raw JSON from Jira (batched)
- `data/processed/` - JSONL training datasets
- `data/checkpoints/` - Progress bookmarks

---

## Edge Cases Handled

### Network Issues
| Issue | Solution |
|-------|----------|
| Timeout (>30s) | Retry with exponential backoff |
| HTTP 429 | Wait 60s, retry |
| HTTP 5xx | Retry up to 5 times |
| Connection reset | Session auto-reconnects |
| DNS failure | Caught and retried |

### Data Quality Issues
| Issue | Solution |
|-------|----------|
| Missing fields | Return "Unknown" |
| Null values | Safe `.get()` access |
| Empty description | Skip issue |
| HTML in text | Regex removes tags |
| Malformed JSON | Try-except with logging |
| Unicode characters | UTF-8 encoding |

### Operational Issues
| Issue | Solution |
|-------|----------|
| Ctrl+C | Graceful shutdown, progress saved |
| Disk full | Exception caught before crash |
| Crash mid-batch | Checkpoint saved after each batch |
| Out of memory | Batch processing (100 issues at time) |


---

## Optimization Strategies

### Why These Choices

| Strategy | Reason |
|----------|--------|
| **REST API** | Structured JSON, official, faster, more reliable than HTML scraping |
| **Batch processing (100/request)** | Jira allows 1000 max, but 100 is sweet spot for stability |
| **Session reuse** | Reuses TCP connections (20-30% faster) |
| **Regex for text cleaning** | 10x faster than HTML parsers like BeautifulSoup |
| **Incremental saves** | Prevents data loss (save after each batch, not at end) |
| **JSONL format** | Line-delimited, streamable, LLM-standard |
| **Separate scraper/transformer** | Clean architecture, can run independently |

### Performance

- **Scraping:** 100-200 issues/min (network-limited by Jira rate limits)
- **Transformation:** 1000-2000 issues/min (CPU-limited, regex operations)
- **Total time (65k issues):** 1-2 hours

### Memory Efficient
- Processes 100 issues at a time (not entire dataset)
- ~50-100 MB scraper, ~100-200 MB transformer
- Can run on systems with <500 MB available RAM

---

## Output Format
JSONL


### Task Types (4 per issue)
1. **Summarization** - description â†’ title
2. **Status Classification** - title+desc â†’ status (Open/Resolved/Closed)
3. **Priority Classification** - title+desc â†’ priority (Critical/Major/Minor)
4. **QnA** - context+question â†’ answer

**Total output:** ~260,000 training examples from ~65,000 issues

---

## Future Improvements

### Short-term
- Logging module (replace print)
- Progress bar (tqdm)
- Unit tests
- Config validation
- Data quality scoring

### Medium-term
- Parallel scraping (asyncio)
- Database storage 
- Deduplication
- Filtering by date/status/priority

### Long-term
- Apache Spark integration (distributed transformation)
- Multi-source support (GitHub, GitLab)

### Why Not Implemented
- **Parallel scraping:** Rate limiting makes it risky
- **Spark:** Overkill for 65k issues, adds complexity
- **Database:** JSON files sufficient for this scale
- **Logging:** Print statements fine for assignment

---

---

## FAQ

**Q: How does it resume from interruption?**
A: After scraping each batch (100 issues), checkpoint saves position to JSON. On restart, checkpoint is loaded and scraping resumes from that position.

**Q: What happens if there's a network error?**
A: Retry logic tries up to 5 times with exponential backoff (2s, 4s, 8s, 16s, 32s). For HTTP 429 (rate limit), waits 60s specifically.

**Q: How is data loss prevented?**
A: Batches are saved to disk immediately after fetching (not at end). Checkpoint saved after each batch. If crash occurs, only current batch is lost (95% safe).

**Q: Why REST API and not HTML scraping?**
A: REST API gives structured JSON, is official/stable, faster, and doesn't require HTML parsing. HTML scraping would be fragile.

**Q: Can I change projects or batch size?**
A: Yes, edit `config.py` - PROJECTS list, MAX_RESULTS_PER_PAGE, retry settings, etc.

**Q: How much time does full scrape take?**
A: 1-2 hours for all 3 projects (~65k issues), network-dependent. Jira rate limits are the main bottleneck.

---

## GitHub

Repository: [webScrapingTutor](https://github.com/nimishgiras123/webScrapingTutor)

---

**Last Updated:** November 2, 2025


